{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:18:54.445152Z","iopub.status.busy":"2023-05-16T09:18:54.444495Z","iopub.status.idle":"2023-05-16T09:19:06.360009Z","shell.execute_reply":"2023-05-16T09:19:06.359104Z","shell.execute_reply.started":"2023-05-16T09:18:54.445050Z"},"trusted":true},"outputs":[],"source":["# Required imports\n","import numpy as np \n","import pandas as pd \n","import os\n","from fastai.vision.all import *\n","from fastai.text.all import *\n","from pathlib import Path\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision\n","import torchtext\n","from torchtext.data import get_tokenizer   # for tokenization\n","from collections import Counter     # for tokenizer\n","import torchvision.transforms as T\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","import PIL\n","from PIL import Image\n","from nltk.translate import bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","captions_path = '/kaggle/input/flickr8k/captions.txt'\n","images_path = \"../input/flickr8k/Images/\"\n","\n","tokenizer = get_tokenizer(\"basic_english\")  \n","token_counter = Counter()\n","\n","\n","    "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:06.474234Z","iopub.status.busy":"2023-05-16T09:19:06.473717Z","iopub.status.idle":"2023-05-16T09:19:09.617455Z","shell.execute_reply":"2023-05-16T09:19:09.616638Z","shell.execute_reply.started":"2023-05-16T09:19:06.474194Z"},"trusted":true},"outputs":[],"source":["class my_dictionary(dict): \n","    def __init__(self): \n","        self = dict() \n","          \n","    def add(self, key, value): \n","        if key not in self.keys():\n","            self[key] = [value]\n","        else:\n","            self[key].append(value)\n","        \n","descriptors = my_dictionary() \n","\n","for i in range(len(df)):\n","    img_id = df.iloc[i, 0]\n","    sentence = (\"<start> \" + df.iloc[i, 1] + \" <end>\").split()\n","    descriptors.add(img_id, sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_image(img, title=None):\n","    \n","    # unnormalize\n","    img[0] *= 0.229\n","    img[1] *= 0.224\n","    img[2] *= 0.225\n","    img[0] += 0.485\n","    img[1] += 0.456\n","    img[2] += 0.406\n","    \n","    img = img.numpy().transpose((1, 2, 0))\n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","        \n","    plt.pause(0.001) "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:09.620148Z","iopub.status.busy":"2023-05-16T09:19:09.619871Z","iopub.status.idle":"2023-05-16T09:19:13.029706Z","shell.execute_reply":"2023-05-16T09:19:13.028903Z","shell.execute_reply.started":"2023-05-16T09:19:09.620110Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb540f0a432543818eeab27444789e23","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/104M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["class textVocab:\n","    def __init__(self):\n","        self.itos = {0:\"<PAD>\", 1:\"<start>\", 2:\"<end>\", 3:\"<UNK>\"}\n","        self.stoi = {b:a for a, b in self.itos.items()}   \n","        self.min_freq = 1\n","        self.tokenizer = get_tokenizer(\"basic_english\") \n","        self.token_counter = Counter()\n","        \n","    def __len__(self):\n","        return len(self.itos)\n","    \n","    def tokenize(self, text):\n","        return self.tokenizer(text)\n","    \n","    def numericalize(self, text):\n","        tokens_list = self.tokenize(text)      \n","        ans = []\n","        for token in tokens_list:\n","            if token in self.stoi.keys():\n","                ans.append(self.stoi[token]) \n","            else:\n","                ans.append(self.stoi[\"<UNK>\"])\n","        return ans   \n","    \n","    def build_vocab(self, sentence_list):\n","        word_count = 4\n","        for sentence in sentence_list:            \n","            tokens = self.tokenizer(sentence)\n","            token_counter.update(tokens)\n","            for token in tokens:\n","                if token_counter[token] >= self.min_freq and token not in self.stoi.keys():\n","                    self.stoi[token] = word_count\n","                    self.itos[word_count] = token\n","                    word_count += 1\n","                    \n","inception = models.inception_v3(pretrained=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:14.024827Z","iopub.status.busy":"2023-05-16T09:19:14.024110Z","iopub.status.idle":"2023-05-16T09:19:14.033585Z","shell.execute_reply":"2023-05-16T09:19:14.032622Z","shell.execute_reply.started":"2023-05-16T09:19:14.024785Z"},"trusted":true},"outputs":[],"source":["train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","# we have to input the test and train indices to create the samplers."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:14.037700Z","iopub.status.busy":"2023-05-16T09:19:14.037383Z","iopub.status.idle":"2023-05-16T09:19:14.045874Z","shell.execute_reply":"2023-05-16T09:19:14.045113Z","shell.execute_reply.started":"2023-05-16T09:19:14.037664Z"},"trusted":true},"outputs":[],"source":["dls = torch.utils.data.DataLoader(dataset, \n","                                           batch_size=batch_size, shuffle=False,\n","                                           collate_fn = Collate_fn(pad_value=pad_value, batch_first = True),\n","                                           sampler=train_sampler)\n","\n","validation_loader = torch.utils.data.DataLoader(dataset, shuffle=False,\n","                                                batch_size=batch_size,\n","                                                collate_fn = Collate_fn(pad_value=pad_value, batch_first = True),\n","                                                sampler=valid_sampler)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:14.047557Z","iopub.status.busy":"2023-05-16T09:19:14.047262Z","iopub.status.idle":"2023-05-16T09:19:15.815098Z","shell.execute_reply":"2023-05-16T09:19:15.814284Z","shell.execute_reply.started":"2023-05-16T09:19:14.047520Z"},"trusted":true},"outputs":[],"source":["dlsItr = iter(dls)\n","batch = next(dlsItr)\n","imgs, captions, img_ids = batch\n","for i in range(batch_size):\n","    img, caption = imgs[i], captions[i]\n","    sentence = [dataset.vocab.itos[token] for token in caption.tolist()]\n","    end_indx = sentence.index('<end>')\n","    sentence = sentence[1:end_indx]\n","    sentence = ' '.join(sentence)\n","    break"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:15.816873Z","iopub.status.busy":"2023-05-16T09:19:15.816532Z","iopub.status.idle":"2023-05-16T09:19:16.220064Z","shell.execute_reply":"2023-05-16T09:19:16.219251Z","shell.execute_reply.started":"2023-05-16T09:19:15.816789Z"},"trusted":true},"outputs":[],"source":["VGG16 = models.VGG16(pretrained=True)\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.my_inception = VGG16Extractor(inception)\n","        \n","    def forward(self, images):        \n","        features = self.my_inception(images) \n","        features = features.permute(0, 2, 3, 1)\n","        features = features.view(features.size(0), -1, features.size(-1))        \n","        return features"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:16.221869Z","iopub.status.busy":"2023-05-16T09:19:16.221577Z","iopub.status.idle":"2023-05-16T09:19:16.230080Z","shell.execute_reply":"2023-05-16T09:19:16.229326Z","shell.execute_reply.started":"2023-05-16T09:19:16.221830Z"},"trusted":true},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n","        super(Attention, self).__init__()\n","        self.attention_dim = attention_dim\n","        self.decoder_att = nn.Linear(decoder_dim, attention_dim) \n","        self.encoder_att = nn.Linear(encoder_dim, attention_dim) \n","        self.full_att = nn.Linear(attention_dim, 1) \n","    \n","    def forward(self, features, hidden_states):\n","        att1 = self.encoder_att(features)   \n","        att2 = self.decoder_att(hidden_states)\n","        combined_states = torch.tanh(att1 + att2.unsqueeze(1))\n","        attention_scores = self.full_att(combined_states)\n","        attention_scores = attention_scores.squeeze(2)\n","        alpha = F.softmax(attention_scores, dim=1)\n","        weighted_encoding = features * alpha.unsqueeze(2)   \n","        weighted_encoding = weighted_encoding.sum(dim=1) \n","        return alpha, weighted_encoding"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:16.231980Z","iopub.status.busy":"2023-05-16T09:19:16.231552Z","iopub.status.idle":"2023-05-16T09:19:16.253511Z","shell.execute_reply":"2023-05-16T09:19:16.252789Z","shell.execute_reply.started":"2023-05-16T09:19:16.231942Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, embed_sz, vocab_sz, att_dim, enc_dim, dec_dim, drop_prob=0.3):\n","        super().__init__()\n","        self.vocab_sz = vocab_sz\n","        self.att_dim = att_dim\n","        self.dec_dim = dec_dim\n","        self.embedding = nn.Embedding(vocab_sz, embed_sz)\n","        self.attention = Attention(enc_dim, dec_dim, att_dim)\n","        self.init_h = nn.Linear(enc_dim, dec_dim)\n","        self.init_c = nn.Linear(enc_dim, dec_dim)\n","        self.lstm_cell = nn.LSTMCell(embed_sz + enc_dim, dec_dim, bias=True)\n","        self.f_beta = nn.Linear(dec_dim, enc_dim)\n","        self.fcn = nn.Linear(dec_dim, vocab_sz)\n","        self.drop = nn.Dropout(drop_prob)\n","    \n","    def forward(self, features, captions):        \n","        embeds = self.embedding(captions)\n","        h, c = self.init_hidden_state(features)\n","        cap_len = len(captions[0]) - 1        \n","        batch_sz = captions.size(0)\n","        num_features = features.size(1)\n","        preds = torch.zeros(batch_sz, cap_len, self.vocab_sz)\n","        alphas = torch.zeros(batch_sz, cap_len, num_features)\n","        for i in range(cap_len):\n","            alpha, att_weights = self.attention(features, h)\n","            lstm_input = torch.cat((embeds[:,i], att_weights), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.fcn(self.drop(h))\n","            preds[:, i] = output\n","            alphas[:, i] = alpha\n","        return preds, alphas\n","    \n","    def generate_caption(self, features, max_len=20, vocab=None):\n","        batch_sz = features.size(0)\n","        h, c = self.init_hidden_state(features)\n","        alphas = []\n","        captions = [vocab.stoi['<start>']]\n","        word = torch.tensor(vocab.stoi['<start>']).view(1, -1)\n","        embeds = self.embedding(word)\n","        for i in range(max_len):\n","            alpha, weighted_encoding = self.attention(features, h)\n","            alphas.append(alpha.cpu().detach().numpy())\n","            lstm_input = torch.cat((embeds[:, 0], weighted_encoding), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.fcn(self.drop(h))\n","            output = output.view(batch_sz, -1)\n","            pred_word_idx = output.argmax(dim=1)\n","            captions.append(pred_word_idx.item())\n","            if vocab.itos[pred_word_idx.item()] == '<end>':\n","                break\n","            embeds = self.embedding(pred_word_idx.unsqueeze(0))\n","        return [vocab.itos[idx] for idx in captions], alphas  \n","    \n","    def init_hidden_state(self, encoder_out):\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-16T09:19:16.255387Z","iopub.status.busy":"2023-05-16T09:19:16.254997Z","iopub.status.idle":"2023-05-16T09:19:16.266443Z","shell.execute_reply":"2023-05-16T09:19:16.265716Z","shell.execute_reply.started":"2023-05-16T09:19:16.255341Z"},"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self, embed_sz, vocab_sz, att_dim, enc_dim, dec_dim, drop_prob=0.3):\n","        super().__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder(\n","            embed_sz = embed_sz,\n","            vocab_sz = vocab_sz,\n","            att_dim = att_dim,\n","            enc_dim = enc_dim,\n","            dec_dim = dec_dim\n","        )\n","    \n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        outputs = self.decoder(features, captions)        \n","        return outputs"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
